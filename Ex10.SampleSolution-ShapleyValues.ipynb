{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Musterl√∂sung / Sample solution \n",
    "## 10th exercise: <font color=\"#C70039\">Interpretable Machine Learning with Shapley Values for image classification</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Date:   04.09.2024\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "This is one implementation example to demo XAI for image classification using the inbuild cifar-10 data set, that you have come across with in exercise 8 already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1MnrZSd06ED"
   },
   "source": [
    "## Imports\n",
    "Import all necessary utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.24 or less",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Uebungen\\lib\\site-packages\\shap\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_explanation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cohorts, Explanation\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# explainers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m other\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Uebungen\\lib\\site-packages\\shap\\_explanation.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mslicer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Alias, Obj, Slicer\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DimensionError\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_general\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpChain\n\u001b[0;32m     16\u001b[0m op_chain_root \u001b[38;5;241m=\u001b[39m OpChain(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshap.Explanation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Uebungen\\lib\\site-packages\\shap\\utils\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_clustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     delta_minimization_order,\n\u001b[0;32m      3\u001b[0m     hclust,\n\u001b[0;32m      4\u001b[0m     hclust_ordering,\n\u001b[0;32m      5\u001b[0m     partition_tree,\n\u001b[0;32m      6\u001b[0m     partition_tree_shuffle,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_general\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     OpChain,\n\u001b[0;32m     10\u001b[0m     approximate_interactions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     suppress_stderr,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_masked_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaskedModel, make_masks\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Uebungen\\lib\\site-packages\\shap\\utils\\_clustering.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m njit\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_progress\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_progress\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpartition_tree\u001b[39m(X, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrelation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Uebungen\\lib\\site-packages\\numba\\__init__.py:55\u001b[0m\n\u001b[0;32m     50\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba requires SciPy version 1.0 or greater. Got SciPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m---> 55\u001b[0m \u001b[43m_ensure_critical_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# END DO NOT MOVE\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_version\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_versions\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\Uebungen\\lib\\site-packages\\numba\\__init__.py:42\u001b[0m, in \u001b[0;36m_ensure_critical_deps\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m numpy_version \u001b[38;5;241m>\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m24\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba needs NumPy 1.24 or less\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Numba needs NumPy 1.24 or less"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from   tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from   tensorflow.keras.models import Sequential\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load build-in dataset\n",
    "take the cifar-10 data set from exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "# reshape and normalize data\n",
    "x_train = x_train.reshape(50000, 32, 32, 3).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 32, 32, 3).astype(\"float32\") / 255\n",
    "y_train = y_train.reshape(50000,)\n",
    "y_test = y_test.reshape(10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple CNN, compile and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(32, 32, 3))\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(128, (3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "# inputs and outputs\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"test_for_shap\")\n",
    "# compile the model\n",
    "model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      optimizer=tf.keras.optimizers.Adam(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "  )\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict on the test set (one image for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class label list\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# example image for each class\n",
    "images_dict = dict()\n",
    "\n",
    "for i, l in enumerate(y_train):\n",
    "    if len(images_dict)==10:\n",
    "        break\n",
    "    if l not in images_dict.keys():\n",
    "        images_dict[l] = x_train[i].reshape((32, 32,3))\n",
    "images_dict = dict(sorted(images_dict.items()))\n",
    "    \n",
    "# example image for each class for test set\n",
    "x_test_dict = dict()\n",
    "for i, l in enumerate(y_test):\n",
    "    if len(x_test_dict)==10:\n",
    "        break\n",
    "    if l not in x_test_dict.keys():\n",
    "        x_test_dict[l] = x_test[i]\n",
    "        \n",
    "# order by class\n",
    "x_test_each_class = [x_test_dict[i] for i in sorted(x_test_dict)]\n",
    "x_test_each_class = np.asarray(x_test_each_class)\n",
    "\n",
    "# Compute predictions\n",
    "predictions = model.predict(x_test_each_class)\n",
    "predicted_class = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "#### plot function\n",
    "define a plot function for actual and predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot actual and predicted class\n",
    "def plot_actual_predicted(images, pred_classes):\n",
    "    fig, axes = plt.subplots(1, 11, figsize=(16, 15))\n",
    "    axes = axes.flatten()\n",
    "  \n",
    "    # plot\n",
    "    ax = axes[0]\n",
    "    dummy_array = np.array([[[0, 0, 0, 0]]], dtype='uint8')\n",
    "    ax.set_title(\"Base reference\")\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(dummy_array, interpolation='nearest')\n",
    "    \n",
    "    # plot image\n",
    "    for k,v in images.items():\n",
    "        ax = axes[k+1]\n",
    "        ax.imshow(v, cmap=plt.cm.binary)\n",
    "        ax.set_title(f\"True: %s \\nPredict: %s\" % (class_names[k], class_names[pred_classes[k]]))\n",
    "        ax.set_axis_off()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XAI using SHAP\n",
    "Now use the SHAP library to generate the Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select background for shap\n",
    "background = x_train[np.random.choice(x_train.shape[0], 1000, replace=False)]\n",
    "\n",
    "# use DeepExplainer to explain predictions of the model\n",
    "explainer = shap.DeepExplainer(model, background)\n",
    "\n",
    "# compute the shapley values\n",
    "shap_values = explainer.shap_values(x_test_each_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot the Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_predicted(images_dict, predicted_class)\n",
    "print()\n",
    "shap.image_plot(shap_values, x_test_each_class * 255)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lime_image.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
