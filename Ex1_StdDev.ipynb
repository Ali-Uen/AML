{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42142d60",
   "metadata": {
    "id": "42142d60"
   },
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ba942",
   "metadata": {
    "id": "756ba942"
   },
   "source": [
    "# 1st exercise: <font color=\"#C70039\">Work with standard deviations for anomaly detection</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Date:   24.10.2023\n",
    "* Student: Ali Ãœnal\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Standard_deviation_diagram.svg/1200px-Standard_deviation_diagram.svg.png\" style=\"float: center;\" width=\"450\">\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**:\n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole.\n",
    "\n",
    "**GENERAL NOTE 2**:\n",
    "* Please, when commenting source code, just use English language only.\n",
    "* When describing an observation please use English language, too\n",
    "* This applies to all exercises throughout this course.  \n",
    "\n",
    "---------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "This notebook allows you for getting into standard deviations as a common technique to detect anomalies when the data is normally distributed.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points.\n",
    "If a task is more challenging and consists of several steps, this is indicated as well.\n",
    "Make sure you have worked down the task list and commented your doings.\n",
    "This should be done by using markdown.<br>\n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date.\n",
    "    * set the date too and remove mine.\n",
    "\n",
    "3. read the entire notebook carefully\n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time.\n",
    "    * understand the outputcburn\n",
    "\n",
    "4. go and find three different data sets on the web\n",
    "    * kaggle.com might be a good source (they also offer an API for data download)\n",
    "    * make sure two of the three data sets are normally distributed\n",
    "    * download one data set that is not normally distributed\n",
    "\n",
    "5. visualize the data\n",
    "\n",
    "6. compute the anomalies\n",
    "---\n",
    "7. visualize the anomalies\n",
    "8. does the 0,3% rule apply?\n",
    "9. what are differences between the normally distributed and the non-normally distributed data sets with respect to the outlier detection?\n",
    "10. which statement can be made and which cannot?\n",
    "-----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c30b564",
   "metadata": {
    "id": "3c30b564",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import lognorm\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import lognorm\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqPX_LNf8dmE",
   "metadata": {
    "id": "aqPX_LNf8dmE"
   },
   "source": [
    "# Define Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sYZSVpJspb-D",
   "metadata": {
    "id": "sYZSVpJspb-D"
   },
   "outputs": [],
   "source": [
    "# Load normalized Data for the first two Datasets\n",
    "df_firstDataset = pd.read_csv(\"SOCR-HeightWeight.csv\")['Height(Inches)']\n",
    "df_secondDataset = pd.read_csv(\"SOCR-HeightWeight.csv\")['Weight(Pounds)']\n",
    "\n",
    "# Load not normalized Data for last Dataset\n",
    "df_thirdDataset = pd.read_csv(\"incomeUS.csv\")['Age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZSQSY_4n6eby",
   "metadata": {
    "id": "ZSQSY_4n6eby"
   },
   "source": [
    "# Test For Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O47k2CfQyg7o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "O47k2CfQyg7o",
    "outputId": "32c2d9cf-715d-4d4e-c376-3a35c58a202b"
   },
   "outputs": [],
   "source": [
    "# Method 1: Plot a histogram for the '?' column\n",
    "plt.hist(df_firstDataset, bins=20, color='skyblue', edgecolor='black')\n",
    "# Method 3: Perform a shapiro wilk test\n",
    "# perform Shapiro-Wilk test for normality\n",
    "# Since the p-value is less than .05, we reject the null hypothesis of the Shapiro-Wilk test.\n",
    "# This means we have sufficient evidence to say that the sample data does not come from a normal distribution.\n",
    "# shapiro(df_firstDataset)\n",
    "\n",
    "# Method 2: Create a Q-Q plot\n",
    "fig = sm.qqplot(df_secondDataset, line='45')\n",
    "plt.show()\n",
    "\n",
    "# Method 4: Perform a Kolmogorov-Smirnov Test\n",
    "kstest(df_firstDataset, 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C4wazRoD9TeV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 879
    },
    "id": "C4wazRoD9TeV",
    "outputId": "03dd46a9-3d8b-4122-b33e-b323410a433f"
   },
   "outputs": [],
   "source": [
    "plt.hist(df_secondDataset, bins=20, color='skyblue', edgecolor='black')\n",
    "\n",
    "fig = sm.qqplot(df_secondDataset, line='45')\n",
    "plt.show()\n",
    "\n",
    "kstest(df_secondDataset, 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QUBDRreI9XTo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897
    },
    "id": "QUBDRreI9XTo",
    "outputId": "bab6f8a9-2589-4b03-b4eb-3d0bd9dd4479"
   },
   "outputs": [],
   "source": [
    "plt.hist(df_thirdDataset, bins=20, color='skyblue', edgecolor='black')\n",
    "\n",
    "fig = sm.qqplot(df_secondDataset, line='45')\n",
    "plt.show()\n",
    "\n",
    "kstest(df_thirdDataset, 'norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hwbAHL0hl1uG",
   "metadata": {
    "id": "hwbAHL0hl1uG",
    "tags": []
   },
   "source": [
    "# Find Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84567403",
   "metadata": {
    "id": "84567403",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to Detection Outlier on one-dimentional datasets.\n",
    "def find_anomalies(random_data):\n",
    "    #define a list to accumlate anomalies\n",
    "    anomalies = []\n",
    "\n",
    "    # Set upper and lower limit to 3 standard deviation\n",
    "    random_data_std = np.std(random_data)\n",
    "    random_data_mean = np.mean(random_data)\n",
    "    anomaly_cut_off = random_data_std * 3\n",
    "\n",
    "    lower_limit  = random_data_mean - anomaly_cut_off\n",
    "    upper_limit = random_data_mean + anomaly_cut_off\n",
    "\n",
    "    print(\"lower limit=\", round(lower_limit,8))\n",
    "    print(\"upper limit=\", round(upper_limit,8))\n",
    "\n",
    "    # Generate outliers list\n",
    "    for outlier in random_data:\n",
    "        if outlier > upper_limit or outlier < lower_limit:\n",
    "            anomalies.append(outlier)\n",
    "\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea28b74",
   "metadata": {
    "id": "0ea28b74",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# multiply and add by random numbers to get some real values\n",
    "# randn generates samples from the normal distribution (important - see below)\n",
    "data = np.random.randn(50000)  * 20 + 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cbe6cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5cbe6cc",
    "outputId": "9a68ceb8-df75-400e-f25c-390469596507",
    "tags": []
   },
   "outputs": [],
   "source": [
    "anomalies_firstDataset = find_anomalies(df_firstDataset)\n",
    "anomalies_secondDataset = find_anomalies(df_secondDataset)\n",
    "anomalies_thirdDataset = find_anomalies(df_thirdDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c895a",
   "metadata": {
    "id": "f84c895a"
   },
   "source": [
    "## Result\n",
    "These anomalies are exceeding the lower and upper 3rd scatter range.\n",
    "Thus, statistically spoken, they do belong to a population size of less than 0,3% of the entire data set!\n",
    "For sure, the above conclusion is true if and only if the data is normally distributed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf3b07",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17cf3b07",
    "outputId": "09ae3b12-5e3b-486c-9f63-ff5300121d92"
   },
   "outputs": [],
   "source": [
    "print(\"Normalized Data, Dataset 1: \" + str(anomalies_firstDataset))\n",
    "print(\"Percentage of anomalies: \" + str(\"{:.5f}\".format(len(anomalies_firstDataset)/len(df_firstDataset)* 100))\n",
    " + \"%; Count of anomalies: \" + str(len(anomalies_firstDataset)) + \"\\n\")\n",
    "\n",
    "print(\"Normalized Data, Dataset 2: \" + str(anomalies_secondDataset))\n",
    "print(\"Percentage of anomalies: \" + str(\"{:.5f}\".format(len(anomalies_secondDataset)/len(df_secondDataset)* 100))\n",
    " + \"%; Count of anomalies: \" + str(len(anomalies_secondDataset)) + \"\\n\")\n",
    "\n",
    "print(\"Not Normalized Data, Dataset 3: \" + str(anomalies_thirdDataset))\n",
    "print(\"Percentage of anomalies: \" + str(\"{:.5f}\".format(len(anomalies_thirdDataset)/len(df_thirdDataset)* 100))\n",
    " + \"%; Count of anomalies: \" + str(len(anomalies_thirdDataset)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95385d5a-94a7-40d5-bace-15b7fda06913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(8-10)\n",
    "#The not normalized data set has the slightly more anomalies than the normalized data sets\n",
    "#Both the normalized data sets full fill the the 0,3% rule, but the not normalized data set is slighlty above the 0,3% Rule.\n",
    "#Using the histogram for viusalizing outliners is only sensible when the data set is normalized"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
