{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4IBezd74Vdz"
   },
   "source": [
    "<img src=\"https://www.th-koeln.de/img/logo.svg\" style=\"float:right;\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzlffEai4Vd0"
   },
   "source": [
    "# 9th exercise: <font color=\"#C70039\">Interpretable Machine Learning by means of Partial Dependence (PDP) and Individual Conditional Expectation (ICE) Plots</font>\n",
    "* Course: AML\n",
    "* Lecturer: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Author of notebook: <a href=\"https://www.gernotheisenberg.de/\">Gernot Heisenberg</a>\n",
    "* Date:   04.08.2025\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_partial_dependence_003.png\" style=\"float: center;\" width=\"800\">\n",
    "\n",
    "---------------------------------\n",
    "**GENERAL NOTE 1**:\n",
    "Please make sure you are reading the entire notebook, since it contains a lot of information on your tasks (e.g. regarding the set of certain paramaters or a specific computational trick), and the written mark downs as well as comments contain a lot of information on how things work together as a whole.\n",
    "\n",
    "**GENERAL NOTE 2**:\n",
    "* Please, when commenting source code, just use English language only.\n",
    "* When describing an observation please use English language, too.\n",
    "* This applies to all exercises throughout this course.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"ce33ff\">DESCRIPTION</font>:\n",
    "\n",
    "Partial dependence plots (PDP) and individual conditional expectation (ICE) plots can be used to visualize and analyze the interaction between the target response and a set of input features of interest.\n",
    "Both PDPs [H2009] and ICEs [G2015] assume that the input features of interest are independent from the complement features and this assumption is often violated in practice. Thus, in the case of correlated features, we will create absurd data points to compute the PDP/ICE.\n",
    "\n",
    "[H2009]\n",
    "T. Hastie, R. Tibshirani and J. Friedman, The Elements of Statistical Learning, Second Edition, Section 10.13.2, Springer, 2009.\n",
    "\n",
    "[G2015]\n",
    "A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin, “Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation” Journal of Computational and Graphical Statistics, 24(1): 44-65, Springer, 2015.\n",
    "\n",
    "---------------------------------\n",
    "\n",
    "### <font color=\"FFC300\">TASKS</font>:\n",
    "The tasks that you need to work on within this notebook are always indicated below as bullet points.\n",
    "If a task is more challenging and consists of several steps, this is indicated as well.\n",
    "Make sure you have worked down the task list and commented your doings.\n",
    "This should be done by using markdown.<br>\n",
    "<font color=red>Make sure you don't forget to specify your name and your matriculation number in the notebook.</font>\n",
    "\n",
    "**YOUR TASKS in this exercise are as follows**:\n",
    "1. import the notebook to Google Colab or use your local machine.\n",
    "2. make sure you specified you name and your matriculation number in the header below my name and date.\n",
    "    * set the date too and remove mine.\n",
    "3. read the entire notebook carefully\n",
    "    * add comments whereever you feel it necessary for better understanding\n",
    "    * run the notebook for the first time.\n",
    "    * try to follow the interpretations by printing out the decision tree and look for the feature patterns that the PDPs indicate.\n",
    "\n",
    "**PART I**<br>\n",
    "4. download an interesting data set from Kaggle and do the preprocessing.<br>\n",
    "5. change the classifier according to the data set. The more blackbox the better.<br>\n",
    "6. use PDP to identify the most relevant features explaining the target response of the data set.<br>\n",
    "7. comment your entire code and your findings.<br>  \n",
    "\n",
    "**PART II**<br>\n",
    "8. use the data set and the classifer from steps 4 and 5<br>\n",
    "9. plot ICE curves with parameter (kind='both')<br>\n",
    "10. comment your entire code and your findings.<br>  \n",
    "\n",
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAz9pNCB4Vd2",
    "tags": []
   },
   "source": [
    "# <font color=\"ce33ff\">PART I (Partial Dependence Plots)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1MnrZSd06ED"
   },
   "source": [
    "## Imports\n",
    "Import all necessary python utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iUO8Wbn4Vd4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02umKYoK4Vd4"
   },
   "source": [
    "## Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GK-oQtIt4Vd4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/FIFA/FIFA.Statistics.2018.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-d1Qw8a4Vd4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all features are:\n",
    "print(data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_a7qB6g4Vd5"
   },
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1SjXs5Y4Vd5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert from string “Yes”/”No” to binary\n",
    "y = (data['Man of the Match'] == 'Yes')\n",
    "\n",
    "feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\n",
    "\n",
    "x = data[feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bglQ6vEL4Vd5"
   },
   "source": [
    "## Train the classifier\n",
    "\n",
    "Start with a simple decision tree model.\n",
    "<font color=red>Note:</font> The calculation of a partial dependence can happen obviously, only after a model has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERfePYhk4Vd6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(x, y, random_state=1)\n",
    "tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyYktAwW4Vd6"
   },
   "source": [
    "## Partial Dependence Plots (PDP)\n",
    "#### read the API reference guide for further possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wd2vdPSWccyX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_name = 'Goal Scored'\n",
    "\n",
    "# draw the Partial Dependence Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    estimator=tree_model,\n",
    "    X=val_x,\n",
    "    features=[feature_name],\n",
    "    kind=\"average\",  # alternative: \"individual\" for ICE-Plots\n",
    "    ax=ax\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBTAmuuQ4Vd7"
   },
   "source": [
    "A few things are worth to be pointed out for interpreting this plot.\n",
    "\n",
    "The y-axis is interpreted as change in the prediction from what it would be predicted at the baseline or leftmost value.\n",
    "\n",
    "From this particular graph you can interpret, that scoring one goal substantially increases the chances of winning \"Man of The Match.\"\n",
    "But extra goals beyond that show little to no impact on predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpxSlfRjc6Gz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_name = 'Distance Covered (Kms)'\n",
    "\n",
    "# draw PDP\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    estimator=tree_model,\n",
    "    X=val_x,\n",
    "    features=[feature_name],\n",
    "    kind=\"average\",  # \"individual\" for ICE-Plots\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6ALxgkX4Vd7"
   },
   "source": [
    "In this PDP plot you will see the ticks on the x-axis as depicting the real data samples.\n",
    "\n",
    "This PDP plot seems to be too simple to represent reality.\n",
    "Maybe that's because the model is so simple. Print the decision tree to compare that finding to the decision tree structure.\n",
    "For the purpose of\n",
    "Let's back up our theory and do the same plot with a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EsZ0MNvS4Vd7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a new model: Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=0).fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1caRUK24Vd7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature = 'Distance Covered (Kms)'\n",
    "\n",
    "# PDP with ICE lines\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    estimator=rf_model,\n",
    "    X=val_x,\n",
    "    features=[feature],\n",
    "    kind='individual',        # ICE\n",
    "    grid_resolution=100,      # number of points on x-axis\n",
    "    subsample=100,            # number of ICE lines\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "plt.title(\"Partial Dependence: Distance Covered (Kms)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2CjE1mu4Vd8"
   },
   "source": [
    "<font color=red>Interpretation:</font>\n",
    "This model states that it is more likely to win \"Man of the Match\" if the players run a total of about 100 km during the match. More running leads to lower predictions.\n",
    "In general, the smoother shape of this curve seems more plausible than the step function of the decision tree model.\n",
    "However, this data set is much too small. One should be very careful when interpreting a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dnFQuki4Vd8"
   },
   "source": [
    "## 2D Partial Depedence Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvY2ybtQ4Vd8"
   },
   "source": [
    "Now, plotting the PDP for two features can be done by using the **pdp_interact** and **pdp_interact_plot** functions.\n",
    "\n",
    "First, switch back to the simple decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDnpLuw04Vd8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(x, y, random_state=1)\n",
    "tree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXna4Gmd4Vd9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Similar to previous PDP plot\n",
    "\n",
    "# Define the pair of features to visualize their interaction\n",
    "features_to_plot = [('Goal Scored', 'Distance Covered (Kms)')]\n",
    "\n",
    "# Create the Partial Dependence Plot (PDP) for the interaction\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    estimator=tree_model,\n",
    "    X=val_x,\n",
    "    features=features_to_plot,\n",
    "    kind='average',           # Interaction PDPs only support 'average'\n",
    "    grid_resolution=20,       # Controls the granularity of the plot\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnNxs4Dd4Vd9"
   },
   "source": [
    "<font color=red>Interpretation:</font>\n",
    "This **2D PDP** shows predictions for any combination of **Goals Scored** and **Distance Covered (Kms)**.\n",
    "\n",
    "For example, it seems to yield the highest predictions when a team scores at least one (1) goal and they run a total distance close to 100km.\n",
    "If the players score 0 goals, the covered distance does not matter.\n",
    "\n",
    "Try to see this by tracing through the decision tree with 0 goals!\n",
    "\n",
    "But distance can impact predictions if the players score goals.\n",
    "Make sure you can see this from the 2D PDP plot.\n",
    "\n",
    "Can you find this pattern in the decision tree too?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNLmwP6W4Vd9",
    "tags": []
   },
   "source": [
    "# <font color=\"ce33ff\">PART II (Individual Conditional Expectation)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-1I3PpV4Vd9"
   },
   "source": [
    "ICE is also a model-agnostic method that can be applied to any model.\n",
    "In fact, it is basically the same concept as PDP but is different in that it displays the marginal effect of feature(s)\n",
    "for each instance instead of calculating the average effect in a overall data context as the PDP does.\n",
    "Thus, it can understood as the equivalent to a PDP for individual data instances.\n",
    "Visually, an ICE plot displays the dependence of the prediction on a feature for each instance separately,\n",
    "resulting in one line per instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9orpUUpH4Vd9"
   },
   "source": [
    "There are multiple packages and libraries that can be used to compute ICE plots.\n",
    "\n",
    "The PartialDependenceDisplay function in the sklearn.inspection module, the PyCEBox package and H2O package’s ice_plot function are available.\n",
    "\n",
    "Let’s take a look at an example in Sklearn’s documentation (https://scikit-learn.org/stable/modules/partial_dependence.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d48mDRa64Vd9"
   },
   "source": [
    "## Imports\n",
    "Import all necessary python utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vGvqgsg4Vd-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# make sure you have installed scikit-learn of version > 1.0\n",
    "# since the method from_estimator() is not available in previous versions\n",
    "from sklearn.inspection import PartialDependenceDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7M7uGV_4Vd-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read some inbuild data set as part of the Sklearn data sets being offered\n",
    "# To get more information on the data set please refer to\n",
    "''' https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_hastie_10_2.html '''\n",
    "\n",
    "x, y = make_hastie_10_2(random_state=0) # set a seed with random_state\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(x, y)\n",
    "features = [0, 1] #features x and y\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(clf, x, features, kind='individual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82T7sqHR4Vd-"
   },
   "source": [
    "It is evident that, similar to PDPs, ICE curves can be computed only after a model has been trained.\n",
    "\n",
    "If you specify the parameter kind='both', then a PDP and an ICE curve is plotted in one canvas at the same time.\n",
    "This will be meaningful when looking at both, the marginal average effect and marginal individual effects at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vDRA5kJ4Vd-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(clf, x, features, kind='both')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
